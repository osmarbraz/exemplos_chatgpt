{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8vDQWTuolo794KKZqn93g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_chatgpt/blob/main/ExemploChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de uso do ChatGPT\n",
        "\n",
        "\n",
        "Documentação: https://platform.openai.com/docs/introduction\n",
        "\n",
        "Tokenizador: https://platform.openai.com/tokenizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "## Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "# 1 - Instalação de bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Openai"
      ],
      "metadata": {
        "id": "w8vYqCacOG0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ln24P6OFPd",
        "outputId": "ef38301e-57a4-40b5-883e-59e0d8118194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.5.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67620 sha256=53fb75fbc618f06b5d06908e9cc2948fe7e1090855612ae81baedc087af04cb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/47/99/8273a59fbd59c303e8ff175416d5c1c9c03a2e83ebf7525a99\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.26.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "# 2 - ChatGPT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chave API"
      ],
      "metadata": {
        "id": "SE-NuaTwBBOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No seu código Python, importe o módulo requests e defina as suas credenciais da API:"
      ],
      "metadata": {
        "id": "LON24CgR_Rg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = '<coloque sua chave aqui!'"
      ],
      "metadata": {
        "id": "YG4n5qsLCRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Registra a chave nas variáveis de ambiente."
      ],
      "metadata": {
        "id": "zS__WQbKRVbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Configura a variável de ambiente\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ],
      "metadata": {
        "id": "H6rD2X50REZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração"
      ],
      "metadata": {
        "id": "1biCIbO7BI_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define o modelo a ser utilizado.\n",
        "\n",
        "Modelos:\n",
        "\n",
        "*  `text-davinci-003` -\tModelo GPT-3 mais capaz. Pode fazer qualquer tarefa que os outros modelos podem fazer, muitas vezes com maior qualidade, saída mais longa e melhor seguimento de instruções. Também suporta a inserção de conclusões no texto.\tCusto: 4.000 fichas. Treinado\tAté junho de 2021.\n",
        "\n",
        "*  `text-curie-001` -\tMuito capaz, mas mais rápido e com custo menor que Davinci.\tCusto: 2.048 fichas. Treinado até outubro de 2019.\n",
        "\n",
        "*   `texto-babbage-001`\t- Capaz de tarefas diretas, muito rápidas e de baixo custo.\tCusto: 2.048 fichas. Treinado até outubro de 2019.\n",
        "\n",
        "*   `text-ada-001` -\tCapaz de tarefas muito simples, geralmente o modelo mais rápido da série GPT-3 e de menor custo.\tCusto 2.048 fichas. Treinado\taté outubro de 2019.\n"
      ],
      "metadata": {
        "id": "xuJ56dvXZJST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_engine  = 'text-davinci-003'"
      ],
      "metadata": {
        "id": "TlXmqJxnZIFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplos"
      ],
      "metadata": {
        "id": "IFGQJCXlQLs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1 - Incoerência semântica - pt-br"
      ],
      "metadata": {
        "id": "RtPqrN1rAGcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = \"O texto é coerente \\\"Como enfileirar elementos em uma pilha?\\\"\""
      ],
      "metadata": {
        "id": "QZ1t8ZL9_Oal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "completions = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=pergunta,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "u2UYbyM3_WVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibe a resposta."
      ],
      "metadata": {
        "id": "WJ7QU0x3Y_fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = completions.choices[0].text\n",
        "print(message.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_vPgi6eYhAf",
        "outputId": "657de543-4b79-4159-9a91-45c31cf5f627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R:\n",
            "\n",
            "Para enfileirar elementos em uma pilha, você deve usar o método push() para adicionar elementos ao topo da pilha. Você também pode usar o método pop() para remover elementos do topo da pilha. Você pode usar o método peek() para verificar o elemento no topo da pilha sem removê-lo. Você também pode usar o método size() para verificar o tamanho da pilha.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2 - Incoerência semântica"
      ],
      "metadata": {
        "id": "qdrxaSA3Cn6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = \"The text is coherent \\\"I have doubts about the operations in the data structure stack. At what end can i enqueue an element?\\\"\""
      ],
      "metadata": {
        "id": "PA312b3wCn6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "completions = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=pergunta,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "kBTchrSTCn6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibe a resposta."
      ],
      "metadata": {
        "id": "SbtBwAXlCn6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = completions.choices[0].text\n",
        "print(message.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4495062e-8282-4a4d-fdf0-afbb8e91ec3f",
        "id": "VGGa7VpACn6R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text is asking a question about the operations of a data structure stack, specifically at what end an element can be enqueued. The text is coherent and understandable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 3 - Incoerência sintática"
      ],
      "metadata": {
        "id": "3Hd26DTtAMrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = \"The text is coherent \\\"A stack is a data structure, she allows operations on only one end referred to as the top.\\\"\""
      ],
      "metadata": {
        "id": "7hXE4Nd0AO9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "completions = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=pergunta,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "dxC_JOAWAOKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibe a resposta."
      ],
      "metadata": {
        "id": "P_Aw9jlcARf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = completions.choices[0].text\n",
        "print(message.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725e74d5-aa85-471d-ac58-4eaa97ff0d9d",
        "id": "rQPUs16JARf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This sentence is coherent because it clearly explains what a stack is and how it works. It states that a stack is a data structure and that it allows operations on only one end, referred to as the top.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 4 - Incoerência pragmática"
      ],
      "metadata": {
        "id": "2WQKmFZqAU6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = \"The text is coherent \\\"Can you lend me your data structure book? The day is beautiful, let’s go to the beach?\\\"\""
      ],
      "metadata": {
        "id": "v1aOtDYhAU6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "completions = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=pergunta,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "jbekO1HWAU6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibe a resposta."
      ],
      "metadata": {
        "id": "dSQLB2lgAU6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = completions.choices[0].text\n",
        "print(message.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b246f072-d034-4593-8314-fd73f05f709e",
        "id": "ckqu8pNQAU6j"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, the text is not coherent. The two sentences do not relate to each other in any way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 5 - Incoerência estilistica"
      ],
      "metadata": {
        "id": "SWf9N8F1CLmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = \"The text is coherent \\\"Dear Antonio, At this moment I want to express my deepest feelings for your mother having passed away.\\\"\""
      ],
      "metadata": {
        "id": "N9d4_0JABQyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "completions = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=pergunta,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "DatrfnFkCVZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibe a resposta."
      ],
      "metadata": {
        "id": "_rGtJPl3CVZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = completions.choices[0].text\n",
        "print(message.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443cc701-c73e-400c-978d-b4d945a74fa9",
        "id": "OwGNpvcGCVZK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text is coherent and expresses the writer's feelings of sympathy for Antonio's mother's passing.\n"
          ]
        }
      ]
    }
  ]
}